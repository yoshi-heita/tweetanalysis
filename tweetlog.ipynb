{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Recent search** from *Twitter API(Standard)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import datetime\n",
    "import time\n",
    "# import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'Hiragino Sans GB' # matplotlibで描画するフォント設定\n",
    "# rcParams['font.family'] = 'meiryo' # Windowsの場合はMeiryoを指定\n",
    "rcParams['figure.autolayout'] = True # レイアウトの自動調整を利用\n",
    "rcParams['figure.facecolor'] = 'white' # グラフの背景色を白へ変更\n",
    "\n",
    "import seaborn as sns; sns.set(font='Hiragino Sans GB')\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import MeCab\n",
    "from gensim import corpora, models\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "from copy import copy, deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://developer.twitter.com/en/docs/tutorials/analyze-past-conversations\n",
    "# https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/master/Recent-Search/recent_search.py\n",
    "# https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query\n",
    "\n",
    "# To set your enviornment variables in your terminal run the following line:\n",
    "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "\n",
    "# def auth():\n",
    "#     return os.environ.get(\"BEARER_TOKEN\")\n",
    "\n",
    "def create_url(QUERY, MAX_RESULTS):\n",
    "    # クエリ条件：指定のワードを含む、リツイートを除く、ｂｏｔと思われるユーザーのツイートを除く\n",
    "    query = QUERY\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "#     tweet_fields = \"tweet.fields=author_id\"\n",
    "    tweet_fields = \"tweet.fields=author_id,id,text,created_at\"\n",
    "    max_results = MAX_RESULTS\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?query={}&{}&{}\".format(\n",
    "        query, tweet_fields, max_results\n",
    "    )\n",
    "    return url\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "def connect_to_endpoint(url, headers):\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "#     print('status code:', str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def get_tweet(BEARER_TOKEN, MAX_RESULTS, QUERY):\n",
    "    bearer_token = BEARER_TOKEN\n",
    "    url = create_url(QUERY, MAX_RESULTS)\n",
    "    headers = create_headers(bearer_token)\n",
    "    json_response = connect_to_endpoint(url, headers)\n",
    "    json_dumps = json.dumps(json_response, indent=4, sort_keys=True)\n",
    "    return ast.literal_eval(re.sub('\\\\n\\s+', '', json_dumps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utc_to_jst(timestamp_utc):\n",
    "    datetime_utc = datetime.datetime.strptime(timestamp_utc + \"+0000\", \"%Y-%m-%d %H:%M:%S.%f%z\")\n",
    "    datetime_jst = datetime_utc.astimezone(datetime.timezone(datetime.timedelta(hours=+9)))\n",
    "    timestamp_jst = datetime.datetime.strftime(datetime_jst, '%Y-%m-%d %H:%M:%S')\n",
    "    return timestamp_jst\n",
    "\n",
    "def shape_data(data):\n",
    "    for i, d in enumerate(data):\n",
    "        # URLの置換\n",
    "        data[i]['text'] = re.sub('[ 　]https://t\\.co/[a-zA-Z0-9]+', ' ', d['text'])\n",
    "        # ユーザー名の置換\n",
    "        data[i]['text'] = re.sub('[ 　]?@[a-zA-Z0-9_]+[ 　]', ' ', d['text'])\n",
    "        # 絵文字の置換\n",
    "        data[i]['text'] = d['text'].encode('cp932',errors='ignore').decode('cp932')\n",
    "#         # ハッシュタグの置換\n",
    "#         data[i]['text'] = re.sub('#.+ ', ' ', d['text'])\n",
    "        # 全角スペース、タブ、改行をスペースへ置換\n",
    "        data[i]['text'] = re.sub(r\"[\\u3000\\t\\n]\", \" \", d['text'])\n",
    "        # 日付時刻の変換（UTCからJST）\n",
    "        data[i]['created_at'] = utc_to_jst(d['created_at'].replace('T', ' ')[:-1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BEARER_TOKEN = \"ここにBearertokenを入れます\"\n",
    "MAX_RESULTS = \"max_results=100\" # A number between 10 and 100.\n",
    "\n",
    "TARGET_WORDS = [\n",
    "    \"三井ダイレクト\",\n",
    "    \"ソニー損保\",\n",
    "    \"アクサダイレクト\"\n",
    "]\n",
    "QUERY_CONDITIONS = [\n",
    "    \"-is:retweet -(from:ray_takagi OR from:daikiti62382874 OR from:2map27televiman OR from:akari8671 OR from:hokenwalker OR from:usachan_21 OR from:daikiti62382874)\",\n",
    "    \"-is:retweet -(3月9日を OR %22ソニー 損保 cm 女優%22) -(from:08453103 OR from:keith1919 OR from:JK71068190 OR from:2map27televiman from:gooddrive_pr OR from:akari8671 OR from:hokenwalker OR from:up7AGucBlUdebhJ OR from:P42660868 OR from:jk27273809 OR from:Sakiho48692364 OR from:JK56524873 OR from:nini82836031 OR from:Momoko61030593 OR from:cgsrp902twt OR from:JK56524873 OR from:K4XoZ6OPGDtdZGA OR from:Momoko61030593 OR from:Skyrocket_Co)\",\n",
    "    \"-is:retweet -(from:2map27televiman OR from:akari8671 OR from:hokenwalker OR from:aiueo_700_bot OR from:PutiMotor OR from:hirayamaruo)\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "iterator, request_iterator = 0, 0\n",
    "\n",
    "# クエリのlistが終わるまでAPIを叩く\n",
    "for target_word, query_ in zip(TARGET_WORDS, QUERY_CONDITIONS):\n",
    "    next_token = ''\n",
    "    break_flag = False\n",
    "    # 次ページがなくなるまで次ページのクエリを取得\n",
    "    while True:\n",
    "        try:\n",
    "            data['meta']['next_token']\n",
    "        except KeyError: # 次ページがない(next_tokenがない)場合はループを抜ける\n",
    "            del data\n",
    "            break_flag = True\n",
    "        except NameError: # TARGET_WORDS内の各要素で初めてAPIを取得するとき\n",
    "            query = query_\n",
    "        else: # 2ページめ以降の処理\n",
    "            next_token = data['meta']['next_token']\n",
    "            query = query_ + '&next_token=' + next_token\n",
    "        finally:\n",
    "            if break_flag == True: break\n",
    "            QUERY = '{} {}'.format(target_word, query)\n",
    "            data = get_tweet(BEARER_TOKEN, MAX_RESULTS, QUERY)\n",
    "            temp_df = pd.DataFrame(shape_data(data['data']))\n",
    "            temp_df[target_word] = True\n",
    "            df = pd.concat([df, temp_df])\n",
    "            \n",
    "            iterator += data['meta']['result_count']\n",
    "            \n",
    "            request_iterator += 1\n",
    "            if request_iterator >= 180: # 180requestを超えたら止める\n",
    "                print('180リクエストを超えるため、15分間停止します...')\n",
    "                time.sleep(15.01*60) # 15分間（余裕をみてプラス1秒弱）中断\n",
    "                request_iterator = 0\n",
    "                \n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(str(iterator) + '件取得しました。')\n",
    "\n",
    "target_word = ['三井ダイレクト', 'ソニー損保', 'アクサダイレクト']\n",
    "for target in target_word:\n",
    "    df[target].fillna(False, inplace=True)\n",
    "\n",
    "df.to_pickle('./raw_tweetlog.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweetlog = pd.read_pickle('./raw_tweetlog.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同一ツイートを重複して取得しているため、ツイートidのユニークなDataFramewo作成（TARGET_WORDS（会社名）ごとのフラグも集約）\n",
    "dupulicate_target = 'id'\n",
    "uniqueflag_df = raw_tweetlog[raw_tweetlog.duplicated(subset=dupulicate_target, keep=False)].\\\n",
    "                groupby(dupulicate_target).agg(\n",
    "                {'三井ダイレクト': lambda x: True if sum(x) > 0 else False,\n",
    "                'ソニー損保': lambda x: True if sum(x) > 0 else False,\n",
    "                'アクサダイレクト': lambda x: True if sum(x) > 0 else False}).reset_index()\n",
    "\n",
    "# ユニークなDataFrameを参照してツイートがあれば各TARGET_WORDのフラグを上書き、新たなDataFrameを作成\n",
    "df = raw_tweetlog.merge(uniqueflag_df, on=[dupulicate_target], how='left', suffixes=('_',''))\n",
    "target_word = ['三井ダイレクト', 'ソニー損保', 'アクサダイレクト']\n",
    "for target in target_word:\n",
    "    df[target] = df[target].fillna(df[target+'_'])\n",
    "    df = df.drop(target+'_', axis=1)\n",
    "\n",
    "df.drop_duplicates(subset=dupulicate_target, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_at'] = pd.to_datetime(df['created_at'], format='%Y-%m-%d')\n",
    "df['target_count'] = df.apply(lambda x: sum(x[['三井ダイレクト', 'ソニー損保', 'アクサダイレクト']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素解析の関数：Mecabにかけて、名詞・動詞・形容詞を出力する\n",
    "\n",
    "# 表層部がうまく分かち書きされない既知のバグがあるため\n",
    "# その場合はGitHubから最新のソースコードを取得してインストールする\n",
    "# https://qiita.com/rinatz/items/410dd55e98f1eddc8071\n",
    "\n",
    "def mecab_list(sentence):\n",
    "    \"\"\"\n",
    "    引数：解析対象のテキストオブジェクト（文章単位）\n",
    "    戻り値：listへ格納した形態素解析結果\n",
    "    \"\"\"\n",
    "    tagger = MeCab.Tagger('-Ochasen -u /usr/local/lib/mecab/dic/ipadic/user_dic.dic -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(str(sentence))\n",
    "    word_class = []\n",
    "    type_of_word_class = []\n",
    "    \n",
    "    # 除外ワードがある場合は指定する\n",
    "    stopwords = ['し', 'する', 'こと', 'てる', 'ん', 'の', 'て', 'なっ', 'れ', 'さ', 'なる', 'そう', 'い', 'さん',\\\n",
    "                 '9', 'co', 'https', 't', '思っ', 'いる', 'くる', 'ー', 'みたい', '見', '出', '方', '事', '何',\\\n",
    "                 '中', 'ある', '誰か', 'とき', '誰', '人', '私', 'ため', '03']\n",
    "    target_wclass = ['名詞', '動詞', '形容詞']#, '助動詞']              # 抽出したい品詞を指定する\n",
    "    \n",
    "    while node:\n",
    "        word, wclass = node.surface, node.feature.split(',')[0]     #　sentenceから表層形と品詞情報を取り出す\n",
    "        if wclass != u'BOS/EOS' and \\\n",
    "           word not in stopwords and wclass in target_wclass:  # 対象外の表層形を除外、対象の品詞に絞り込み\n",
    "            word_class.append(word)\n",
    "            type_of_word_class.append(wclass)\n",
    "        node = node.next\n",
    "    return pd.Series({'morphene': word_class, 'morphene_type': type_of_word_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 解析器エンジンにデータを投げ、形態素解析を行う\n",
    "df = pd.concat([df, df['text'].apply(mecab_list)], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def daily_tweet(title, data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    temp_df = data.set_index(data['created_at'].map(lambda s: s.strftime('%m/%d'))).\\\n",
    "    groupby(level=0).size()\n",
    "    plt.bar(temp_df.index, temp_df.values)\n",
    "    plt.title(label='\"{}\" を含むツイート'.format(title))\n",
    "\n",
    "    # ツイート総数\n",
    "    plt.text(x=temp_df.index[-1], y=max(temp_df.values)*0.95,\\\n",
    "             s='ツイート総数:'+str(sum(temp_df.values))+'件', ha='center',\\\n",
    "             bbox=dict(boxstyle='round', fc='white', alpha=0.3, ec='gray'))\n",
    "    # 日別件数\n",
    "    [plt.text(x=temp_df.index[i], y=temp_df.values[i], s=temp_df.values[i], ha='center')\\\n",
    "     for i in range(len(temp_df))]\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 品詞ごとの頻度をカウント\n",
    "def word_frequency(data):\n",
    "    documents = data['morphene']\n",
    "    dct = corpora.Dictionary(documents)\n",
    "    # 「出現頻度が1未満の単語」と「0%以上の文書で出現する単語」を排除\n",
    "    # ↑コーパスの中で出現頻度の低すぎる単語と高すぎる単語は、文書間の違いを表せないので特徴語には不適切と考えて除去\n",
    "    dct.filter_extremes(no_below = 3, no_above = 0.8)\n",
    "    # bow_corpus = [dct.doc2bow(d) for d in documents]\n",
    "\n",
    "    word_freq = {x:dct.dfs[y] for x, y in dct.token2id.items()}\n",
    "    word_freq = dict(sorted(word_freq.items(), key=lambda x:x[1], reverse=True))\n",
    "    word_freq_df = pd.DataFrame(data=word_freq.values(), index=word_freq.keys(), columns=['freq']).head(100)\n",
    "\n",
    "    # # BoWに対してTF-IDFによる重み付けを行う\n",
    "    # tfidf_model = models.TfidfModel(bow_corpus)\n",
    "    # tfidf_corpus = tfidf_model[bow_corpus]\n",
    "    \n",
    "    return word_freq_df\n",
    "\n",
    "# ワードクラウドの描画\n",
    "# https://www.takapy.work/entry/2019/01/14/142128\n",
    "def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n",
    "                   title=None, title_size=60, title_color='gray', bg_color='white'):\n",
    "    \n",
    "    # 日本語に対応させるためにフォントのパスを指定\n",
    "    f_path = '/System/Library/Fonts/ヒラギノ角ゴシック W1.ttc'\n",
    "    \n",
    "    # wordcloudの生成\n",
    "    wordcloud = WordCloud(background_color=bg_color,\n",
    "#                    stopwords = stopwords,\n",
    "                    font_path=f_path, #日本語対応\n",
    "                    max_words=max_words,\n",
    "                    max_font_size=max_font_size, \n",
    "#                    random_state=42,\n",
    "                    width=800, \n",
    "                    height=400,\n",
    "                    mask=mask)\n",
    "    wordcloud.generate(str(text).replace(\"'\", \"\"))\n",
    "    \n",
    "    plt.figure(figsize=figure_size)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(title, fontdict={'size': title_size, \n",
    "                               'color': title_color, \n",
    "                               'verticalalignment': 'bottom'})\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "#　横棒グラフの描画\n",
    "def plot_bar_horizontal(data, figure_size):\n",
    "    plt.figure(figsize=figure_size)\n",
    "    plt.barh(data.index, data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_word_freq_df = word_frequency(df)\n",
    "\n",
    "# 会社ごとのDataFrameを作成\n",
    "md_df = df.loc[df['三井ダイレクト']==True]\n",
    "ss_df = df.loc[df['ソニー損保']==True]\n",
    "ad_df = df.loc[df['アクサダイレクト']==True]\n",
    "\n",
    "for target, data in zip(target_word, [md_df, ss_df, ad_df]):\n",
    "    daily_tweet(target, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(list(company_word_freq_df.index), figure_size=(12, 6), title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for company_df, title in zip([md_df, ss_df, ad_df], target_word):\n",
    "    company_word_freq_df = word_frequency(company_df)\n",
    "    plot_wordcloud(list(company_word_freq_df.index), figure_size=(12, 6), title='')\n",
    "    plot_bar_horizontal(company_word_freq_df[:20][::-1]['freq'], figure_size=(8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起ネットワークの描画\n",
    "# https://qiita.com/hanon/items/a2000da2f70d6c14ca5b\n",
    "\n",
    "def plot_co_occurrence_network(data_morphene, data_morphene_type, text=''):\n",
    "    \n",
    "    node_name = defaultdict(str)\n",
    "    node_idx = defaultdict(int)\n",
    "    node_type = defaultdict(list)\n",
    "    node_count = defaultdict(int)\n",
    "    edge_list = []\n",
    "    cnt = 0\n",
    "    \n",
    "    # DataFrameの形態素・品詞種類の各列からデータを読み込み\n",
    "    for morphene, morphene_type in zip(data_morphene, data_morphene_type):\n",
    "        node_prev = None\n",
    "\n",
    "        for m, m_t in zip(morphene, morphene_type):\n",
    "            # Nodeの処理\n",
    "            if m not in node_name.values():\n",
    "                node_name[cnt] = m\n",
    "                node_idx[m] = cnt\n",
    "                node_count[cnt] = 1\n",
    "                node_type[m_t].append(node_idx[m])\n",
    "                cnt += 1\n",
    "            else:\n",
    "                node_count[node_idx[m]] += 1\n",
    "\n",
    "            # edgeの処理\n",
    "            if (node_prev is not None) & (node_prev != node_idx[m]): # 循環グラフ、有向グラフを回避\n",
    "                edge = (min(node_prev, node_idx[m]), max(node_prev, node_idx[m]))\n",
    "                edge_list.append(edge)\n",
    "            node_prev = node_idx[m]\n",
    "\n",
    "    edge_count = Counter(edge_list)\n",
    "\n",
    "    # Networkxに格納\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from([(idx, {'cnt': node_count[idx]}) for idx in node_name])\n",
    "    G.number_of_nodes(), len(node_name)\n",
    "    G.add_edges_from([(a, b, {'cnt': edge_count[(a, b)]}) for a, b in edge_list])\n",
    "\n",
    "    # Node, Edgeを剪定\n",
    "    G2 = deepcopy(G)\n",
    "    # Node: cnt >= 3で剪定\n",
    "    # 破壊的操作なので、予め破壊用のグラフ(G2)と検索用グラフ(G)を分けておく\n",
    "    for n, attr in G.nodes().items():\n",
    "#         if (attr['cnt'] < 10):\n",
    "        if (attr['cnt'] < 5):\n",
    "            G2.remove_edges_from(list(G.edges(n)))\n",
    "            G2.remove_node(n)\n",
    "\n",
    "    G3 = deepcopy(G2)\n",
    "    # Edge: cnt >= 2で剪定\n",
    "    # EdgeがなくなったNodeは一旦そのまま\n",
    "    for e, attr in G2.edges().items():\n",
    "        if attr['cnt'] < 2:\n",
    "            G3.remove_edge(*e)\n",
    "\n",
    "    G4 = deepcopy(G3)\n",
    "    # EdgeがなくなったNodeを削除\n",
    "    for n in list(G3.nodes()):\n",
    "        if len(G3[n]) == 0:\n",
    "            G4.remove_node(n)\n",
    "\n",
    "    G_result = deepcopy(G4)\n",
    "\n",
    "    pos = nx.layout.spring_layout(G_result, k=0.7, seed=10) # 2次元平面上の座標を計算\n",
    "    labels = {n: node_name[n] for n in pos.keys()} # Nodeに日本語を描画するための辞書\n",
    "    # node_size = [np.log(node_count[n])*400 for n in pos.keys()] # 対数スケール\n",
    "    node_size = [node_count[n]*25 for n in pos.keys()]\n",
    "\n",
    "    edge_alpha = [edge_count[e] for e in G_result.edges()]\n",
    "    edge_colors = [edge_count[e]*2.5 for e in G_result.edges()]\n",
    "    edge_width = [edge_count[e]*0.4 for e in G_result.edges()]\n",
    "\n",
    "    node_dict = dict(zip(G_result.nodes(), node_size))\n",
    "\n",
    "    # 描画\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    # 名詞のNodeを描画\n",
    "    # Nodeを色分けしたいときは、nodelistを使ってNodeのグループ毎に描画関数を繰り返し実行する\n",
    "    # nodelistはグループ毎のNode番号を指定するが、それ以外の引数(posやnode_sizeなど)は全てのNodeについての値を入れる\n",
    "    # 指定出来る色はmatplotlibのcolor exampleを参照\n",
    "    # https://matplotlib.org/examples/color/named_colors.html\n",
    "\n",
    "    node_type_list = ['名詞', '動詞', '形容詞']\n",
    "    node_color_list = ['orange', 'yellowgreen', 'tomato']\n",
    "    \n",
    "    for n_t, n_c in zip(node_type_list, node_color_list):\n",
    "        nx.draw_networkx_nodes(G_result, pos, \n",
    "                               nodelist=[n for n in G_result.nodes() if n in node_type[n_t]], \n",
    "                               node_size=[val for key, val in node_dict.items() if key in \\\n",
    "                                         [n for n in G_result.nodes() if n in node_type[n_t]]], \n",
    "                               node_color=n_c, alpha=0.6, ax=ax)\n",
    "        \n",
    "        # 凡例の出力準備\n",
    "        plt.scatter([], [], c=n_c, alpha=0.5, s=350, label=n_t)\n",
    "\n",
    "    # edgeの色に濃淡をつけたいときは、edge_colorに数値のlistを代入してedge_cmapを使用\n",
    "    # Sequentialなカラーマップから好きなやつを選ぶ\n",
    "    # https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "    # 色の濃淡の具合はedge_vmin, edge_vmaxで調整\n",
    "    nx.draw_networkx_edges(G_result, pos, alpha=0.6,\n",
    "                           width=edge_width, edge_color=edge_colors, \n",
    "                           edge_vmin=0, edge_vmax=10,\n",
    "                           edge_cmap=plt.cm.Blues,ax=ax)\n",
    "    # Nodeにラベルをつけたいときは、以下の関数を使う\n",
    "    # font_familyにPCに入っている日本語フォントを指定してあげると、日本語を描画してくれる\n",
    "    nx.draw_networkx_labels(G_result, pos, labels, font_size=10, font_family=\"Hiragino sans\", ax=ax)\n",
    "\n",
    "    plt.title(text)\n",
    "    \n",
    "    # 凡例表示\n",
    "    plt.legend(scatterpoints=1, frameon=True,\n",
    "           labelspacing=1, title='品詞の種類')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    # fig.patch.set_alpha(0.3)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_co_occurrence_network(df['morphene'].tolist(), df['morphene_type'].tolist())\n",
    "\n",
    "for company_df, text in zip([md_df, ss_df, ad_df], target_word):\n",
    "    plot_co_occurrence_network(company_df['morphene'].tolist(),\\\n",
    "                               company_df['morphene_type'].tolist(), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['target_count']>=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
